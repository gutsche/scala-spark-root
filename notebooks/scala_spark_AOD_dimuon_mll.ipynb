{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run jupyter notebook for Apache Toree: <br>\n",
    "SPARK_OPTS=\"--packages org.diana-hep:spark-root_2.11:0.1.15\" jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.dianahep.sparkroot.experimental._\n",
    "import codegen._\n",
    "import scala.math._\n",
    "val sparknew = spark\n",
    "import sparknew.implicits._\n",
    "//val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n",
    "//import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import root file as dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input root file taken from (on lxplus.cern.ch): <br>\n",
    "eoscp root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12_DR53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball/AODSIM/PU_RD1_START53_V7N-v1/20000/DCF94DC3-42CE-E211-867A-001E67398011.root ./dy_AOD.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val filename = \"file:/home/PlayerOne/Data/EC8239EF-1181-E211-8953-001EC9D80AB9.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find ROOT clas: vector<edm::StoredProductProvenance>. Swallowing...\n",
      "Could not find ROOT clas: vector<unsigned short>. Swallowing...\n"
     ]
    }
   ],
   "source": [
    "val ds = spark.sqlContext.read.option(\"tree\",\"Events\").root(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't print schema here as it's very long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//ds.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select only muon branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsMuons = ds.select(\"recoMuons_muons__RECO_.recoMuons_muons__RECO_obj\").toDF(\"muons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsMuons.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- muons: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- qx3_: integer (nullable = true)\n",
      " |    |    |-- pt_: float (nullable = true)\n",
      " |    |    |-- eta_: float (nullable = true)\n",
      " |    |    |-- phi_: float (nullable = true)\n",
      " |    |    |-- mass_: float (nullable = true)\n",
      " |    |    |-- vertex_: struct (nullable = true)\n",
      " |    |    |    |-- fCoordinates: struct (nullable = true)\n",
      " |    |    |    |    |-- fX: float (nullable = true)\n",
      " |    |    |    |    |-- fY: float (nullable = true)\n",
      " |    |    |    |    |-- fZ: float (nullable = true)\n",
      " |    |    |-- pdgId_: integer (nullable = true)\n",
      " |    |    |-- status_: integer (nullable = true)\n",
      " |    |    |-- innerTrack_: struct (nullable = true)\n",
      " |    |    |    |-- recoMuons_muons__RECO_obj_innerTrack__product_: struct (nullable = true)\n",
      " |    |    |    |    |-- processIndex_: short (nullable = true)\n",
      " |    |    |    |    |-- productIndex_: short (nullable = true)\n",
      " |    |    |    |    |-- elementIndex_: integer (nullable = true)\n",
      " |    |    |-- outerTrack_: struct (nullable = true)\n",
      " |    |    |    |-- recoMuons_muons__RECO_obj_outerTrack__product_: struct (nullable = true)\n",
      " |    |    |    |    |-- processIndex_: short (nullable = true)\n",
      " |    |    |    |    |-- productIndex_: short (nullable = true)\n",
      " |    |    |    |    |-- elementIndex_: integer (nullable = true)\n",
      " |    |    |-- globalTrack_: struct (nullable = true)\n",
      " |    |    |    |-- recoMuons_muons__RECO_obj_globalTrack__product_: struct (nullable = true)\n",
      " |    |    |    |    |-- processIndex_: short (nullable = true)\n",
      " |    |    |    |    |-- productIndex_: short (nullable = true)\n",
      " |    |    |    |    |-- elementIndex_: integer (nullable = true)\n",
      " |    |    |-- recoMuons_muons__RECO_obj_refittedTrackMap_: map (nullable = true)\n",
      " |    |    |    |-- key: integer\n",
      " |    |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |    |-- product_: struct (nullable = true)\n",
      " |    |    |    |    |    |-- processIndex_: short (nullable = true)\n",
      " |    |    |    |    |    |-- productIndex_: short (nullable = true)\n",
      " |    |    |    |    |    |-- elementIndex_: integer (nullable = true)\n",
      " |    |    |-- bestTrackType_: integer (nullable = true)\n",
      " |    |    |-- calEnergy_: struct (nullable = true)\n",
      " |    |    |    |-- tower: float (nullable = true)\n",
      " |    |    |    |-- towerS9: float (nullable = true)\n",
      " |    |    |    |-- em: float (nullable = true)\n",
      " |    |    |    |-- emS9: float (nullable = true)\n",
      " |    |    |    |-- emS25: float (nullable = true)\n",
      " |    |    |    |-- emMax: float (nullable = true)\n",
      " |    |    |    |-- had: float (nullable = true)\n",
      " |    |    |    |-- hadS9: float (nullable = true)\n",
      " |    |    |    |-- hadMax: float (nullable = true)\n",
      " |    |    |    |-- ho: float (nullable = true)\n",
      " |    |    |    |-- hoS9: float (nullable = true)\n",
      " |    |    |    |-- ecal_time: float (nullable = true)\n",
      " |    |    |    |-- ecal_timeError: float (nullable = true)\n",
      " |    |    |    |-- hcal_time: float (nullable = true)\n",
      " |    |    |    |-- hcal_timeError: float (nullable = true)\n",
      " |    |    |    |-- ecal_position: struct (nullable = true)\n",
      " |    |    |    |    |-- fCoordinates: struct (nullable = true)\n",
      " |    |    |    |    |    |-- fX: float (nullable = true)\n",
      " |    |    |    |    |    |-- fY: float (nullable = true)\n",
      " |    |    |    |    |    |-- fZ: float (nullable = true)\n",
      " |    |    |    |-- hcal_position: struct (nullable = true)\n",
      " |    |    |    |    |-- fCoordinates: struct (nullable = true)\n",
      " |    |    |    |    |    |-- fX: float (nullable = true)\n",
      " |    |    |    |    |    |-- fY: float (nullable = true)\n",
      " |    |    |    |    |    |-- fZ: float (nullable = true)\n",
      " |    |    |    |-- ecal_id: struct (nullable = true)\n",
      " |    |    |    |    |-- id_: integer (nullable = true)\n",
      " |    |    |    |-- hcal_id: struct (nullable = true)\n",
      " |    |    |    |    |-- id_: integer (nullable = true)\n",
      " |    |    |-- combinedQuality_: struct (nullable = true)\n",
      " |    |    |    |-- updatedSta: boolean (nullable = true)\n",
      " |    |    |    |-- trkKink: float (nullable = true)\n",
      " |    |    |    |-- glbKink: float (nullable = true)\n",
      " |    |    |    |-- trkRelChi2: float (nullable = true)\n",
      " |    |    |    |-- staRelChi2: float (nullable = true)\n",
      " |    |    |    |-- chi2LocalPosition: float (nullable = true)\n",
      " |    |    |    |-- chi2LocalMomentum: float (nullable = true)\n",
      " |    |    |    |-- localDistance: float (nullable = true)\n",
      " |    |    |    |-- globalDeltaEtaPhi: float (nullable = true)\n",
      " |    |    |    |-- tightMatch: boolean (nullable = true)\n",
      " |    |    |    |-- glbTrackProbability: float (nullable = true)\n",
      " |    |    |    |-- tkKink_position: struct (nullable = true)\n",
      " |    |    |    |    |-- fCoordinates: struct (nullable = true)\n",
      " |    |    |    |    |    |-- fX: double (nullable = true)\n",
      " |    |    |    |    |    |-- fY: double (nullable = true)\n",
      " |    |    |    |    |    |-- fZ: double (nullable = true)\n",
      " |    |    |    |-- glbKink_position: struct (nullable = true)\n",
      " |    |    |    |    |-- fCoordinates: struct (nullable = true)\n",
      " |    |    |    |    |    |-- fX: double (nullable = true)\n",
      " |    |    |    |    |    |-- fY: double (nullable = true)\n",
      " |    |    |    |    |    |-- fZ: double (nullable = true)\n",
      " |    |    |-- time_: struct (nullable = true)\n",
      " |    |    |    |-- nDof: integer (nullable = true)\n",
      " |    |    |    |-- timeAtIpInOut: float (nullable = true)\n",
      " |    |    |    |-- timeAtIpInOutErr: float (nullable = true)\n",
      " |    |    |    |-- timeAtIpOutIn: float (nullable = true)\n",
      " |    |    |    |-- timeAtIpOutInErr: float (nullable = true)\n",
      " |    |    |-- energyValid_: boolean (nullable = true)\n",
      " |    |    |-- matchesValid_: boolean (nullable = true)\n",
      " |    |    |-- isolationValid_: boolean (nullable = true)\n",
      " |    |    |-- pfIsolationValid_: boolean (nullable = true)\n",
      " |    |    |-- qualityValid_: boolean (nullable = true)\n",
      " |    |    |-- caloCompatibility_: float (nullable = true)\n",
      " |    |    |-- isolationR03_: struct (nullable = true)\n",
      " |    |    |    |-- sumPt: float (nullable = true)\n",
      " |    |    |    |-- emEt: float (nullable = true)\n",
      " |    |    |    |-- hadEt: float (nullable = true)\n",
      " |    |    |    |-- hoEt: float (nullable = true)\n",
      " |    |    |    |-- nTracks: integer (nullable = true)\n",
      " |    |    |    |-- nJets: integer (nullable = true)\n",
      " |    |    |    |-- trackerVetoPt: float (nullable = true)\n",
      " |    |    |    |-- emVetoEt: float (nullable = true)\n",
      " |    |    |    |-- hadVetoEt: float (nullable = true)\n",
      " |    |    |    |-- hoVetoEt: float (nullable = true)\n",
      " |    |    |-- isolationR05_: struct (nullable = true)\n",
      " |    |    |    |-- sumPt: float (nullable = true)\n",
      " |    |    |    |-- emEt: float (nullable = true)\n",
      " |    |    |    |-- hadEt: float (nullable = true)\n",
      " |    |    |    |-- hoEt: float (nullable = true)\n",
      " |    |    |    |-- nTracks: integer (nullable = true)\n",
      " |    |    |    |-- nJets: integer (nullable = true)\n",
      " |    |    |    |-- trackerVetoPt: float (nullable = true)\n",
      " |    |    |    |-- emVetoEt: float (nullable = true)\n",
      " |    |    |    |-- hadVetoEt: float (nullable = true)\n",
      " |    |    |    |-- hoVetoEt: float (nullable = true)\n",
      " |    |    |-- pfIsolationR03_: struct (nullable = true)\n",
      " |    |    |    |-- sumChargedHadronPt: float (nullable = true)\n",
      " |    |    |    |-- sumChargedParticlePt: float (nullable = true)\n",
      " |    |    |    |-- sumNeutralHadronEt: float (nullable = true)\n",
      " |    |    |    |-- sumPhotonEt: float (nullable = true)\n",
      " |    |    |    |-- sumNeutralHadronEtHighThreshold: float (nullable = true)\n",
      " |    |    |    |-- sumPhotonEtHighThreshold: float (nullable = true)\n",
      " |    |    |    |-- sumPUPt: float (nullable = true)\n",
      " |    |    |-- pfIsolationR04_: struct (nullable = true)\n",
      " |    |    |    |-- sumChargedHadronPt: float (nullable = true)\n",
      " |    |    |    |-- sumChargedParticlePt: float (nullable = true)\n",
      " |    |    |    |-- sumNeutralHadronEt: float (nullable = true)\n",
      " |    |    |    |-- sumPhotonEt: float (nullable = true)\n",
      " |    |    |    |-- sumNeutralHadronEtHighThreshold: float (nullable = true)\n",
      " |    |    |    |-- sumPhotonEtHighThreshold: float (nullable = true)\n",
      " |    |    |    |-- sumPUPt: float (nullable = true)\n",
      " |    |    |-- type_: integer (nullable = true)\n",
      " |    |    |-- pfP4_: struct (nullable = true)\n",
      " |    |    |    |-- fCoordinates: struct (nullable = true)\n",
      " |    |    |    |    |-- fX: double (nullable = true)\n",
      " |    |    |    |    |-- fY: double (nullable = true)\n",
      " |    |    |    |    |-- fZ: double (nullable = true)\n",
      " |    |    |    |    |-- fT: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dsMuons.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               muons|\n",
      "+--------------------+\n",
      "|[[3,13.94647,-1.1...|\n",
      "|[[-3,23.174383,-1...|\n",
      "|[[-3,18.638512,1....|\n",
      "|[[3,16.0538,-1.08...|\n",
      "|[[3,30.685486,1.5...|\n",
      "|[[-3,3.4471488,2....|\n",
      "|[[3,45.467617,0.8...|\n",
      "|[[3,12.661965,-1....|\n",
      "|[[-3,23.589281,0....|\n",
      "|[[-3,30.441782,-2...|\n",
      "|[[3,27.16122,1.69...|\n",
      "|[[3,38.678493,1.5...|\n",
      "|[[3,44.09437,-1.1...|\n",
      "|[[3,13.94982,-0.1...|\n",
      "|[[3,1.2193049,2.2...|\n",
      "|[[-3,34.696175,0....|\n",
      "|[[3,47.48335,1.59...|\n",
      "|[[3,29.40141,1.01...|\n",
      "|[[3,54.520576,-0....|\n",
      "|[[3,6.000192,-1.1...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dsMuons.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate schema for muon branches\n",
    "\n",
    "This allows us to interact with the nested structure, but generates a lot of specific code.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "case class Record2 (\n",
      "    fX : Float,\n",
      "    fY : Float,\n",
      "    fZ : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record1 (\n",
      "    fCoordinates : Record2\n",
      ")\n",
      "\n",
      "\n",
      "case class Record4 (\n",
      "    processIndex_ : Short,\n",
      "    productIndex_ : Short,\n",
      "    elementIndex_ : Int\n",
      ")\n",
      "\n",
      "\n",
      "case class Record3 (\n",
      "    recoMuons_muons__RECO_obj_innerTrack__product_ : Record4\n",
      ")\n",
      "\n",
      "\n",
      "case class Record6 (\n",
      "    processIndex_ : Short,\n",
      "    productIndex_ : Short,\n",
      "    elementIndex_ : Int\n",
      ")\n",
      "\n",
      "\n",
      "case class Record5 (\n",
      "    recoMuons_muons__RECO_obj_outerTrack__product_ : Record6\n",
      ")\n",
      "\n",
      "\n",
      "case class Record8 (\n",
      "    processIndex_ : Short,\n",
      "    productIndex_ : Short,\n",
      "    elementIndex_ : Int\n",
      ")\n",
      "\n",
      "\n",
      "case class Record7 (\n",
      "    recoMuons_muons__RECO_obj_globalTrack__product_ : Record8\n",
      ")\n",
      "\n",
      "\n",
      "case class Record10 (\n",
      "    processIndex_ : Short,\n",
      "    productIndex_ : Short,\n",
      "    elementIndex_ : Int\n",
      ")\n",
      "\n",
      "\n",
      "case class Record9 (\n",
      "    product_ : Record10\n",
      ")\n",
      "\n",
      "\n",
      "case class Record13 (\n",
      "    fX : Float,\n",
      "    fY : Float,\n",
      "    fZ : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record12 (\n",
      "    fCoordinates : Record13\n",
      ")\n",
      "\n",
      "\n",
      "case class Record15 (\n",
      "    fX : Float,\n",
      "    fY : Float,\n",
      "    fZ : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record14 (\n",
      "    fCoordinates : Record15\n",
      ")\n",
      "\n",
      "\n",
      "case class Record16 (\n",
      "    id_ : Int\n",
      ")\n",
      "\n",
      "\n",
      "case class Record17 (\n",
      "    id_ : Int\n",
      ")\n",
      "\n",
      "\n",
      "case class Record11 (\n",
      "    tower : Float,\n",
      "    towerS9 : Float,\n",
      "    em : Float,\n",
      "    emS9 : Float,\n",
      "    emS25 : Float,\n",
      "    emMax : Float,\n",
      "    had : Float,\n",
      "    hadS9 : Float,\n",
      "    hadMax : Float,\n",
      "    ho : Float,\n",
      "    hoS9 : Float,\n",
      "    ecal_time : Float,\n",
      "    ecal_timeError : Float,\n",
      "    hcal_time : Float,\n",
      "    hcal_timeError : Float,\n",
      "    ecal_position : Record12,\n",
      "    hcal_position : Record14,\n",
      "    ecal_id : Record16,\n",
      "    hcal_id : Record17\n",
      ")\n",
      "\n",
      "\n",
      "case class Record20 (\n",
      "    fX : Double,\n",
      "    fY : Double,\n",
      "    fZ : Double\n",
      ")\n",
      "\n",
      "\n",
      "case class Record19 (\n",
      "    fCoordinates : Record20\n",
      ")\n",
      "\n",
      "\n",
      "case class Record22 (\n",
      "    fX : Double,\n",
      "    fY : Double,\n",
      "    fZ : Double\n",
      ")\n",
      "\n",
      "\n",
      "case class Record21 (\n",
      "    fCoordinates : Record22\n",
      ")\n",
      "\n",
      "\n",
      "case class Record18 (\n",
      "    updatedSta : Boolean,\n",
      "    trkKink : Float,\n",
      "    glbKink : Float,\n",
      "    trkRelChi2 : Float,\n",
      "    staRelChi2 : Float,\n",
      "    chi2LocalPosition : Float,\n",
      "    chi2LocalMomentum : Float,\n",
      "    localDistance : Float,\n",
      "    globalDeltaEtaPhi : Float,\n",
      "    tightMatch : Boolean,\n",
      "    glbTrackProbability : Float,\n",
      "    tkKink_position : Record19,\n",
      "    glbKink_position : Record21\n",
      ")\n",
      "\n",
      "\n",
      "case class Record23 (\n",
      "    nDof : Int,\n",
      "    timeAtIpInOut : Float,\n",
      "    timeAtIpInOutErr : Float,\n",
      "    timeAtIpOutIn : Float,\n",
      "    timeAtIpOutInErr : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record24 (\n",
      "    sumPt : Float,\n",
      "    emEt : Float,\n",
      "    hadEt : Float,\n",
      "    hoEt : Float,\n",
      "    nTracks : Int,\n",
      "    nJets : Int,\n",
      "    trackerVetoPt : Float,\n",
      "    emVetoEt : Float,\n",
      "    hadVetoEt : Float,\n",
      "    hoVetoEt : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record25 (\n",
      "    sumPt : Float,\n",
      "    emEt : Float,\n",
      "    hadEt : Float,\n",
      "    hoEt : Float,\n",
      "    nTracks : Int,\n",
      "    nJets : Int,\n",
      "    trackerVetoPt : Float,\n",
      "    emVetoEt : Float,\n",
      "    hadVetoEt : Float,\n",
      "    hoVetoEt : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record26 (\n",
      "    sumChargedHadronPt : Float,\n",
      "    sumChargedParticlePt : Float,\n",
      "    sumNeutralHadronEt : Float,\n",
      "    sumPhotonEt : Float,\n",
      "    sumNeutralHadronEtHighThreshold : Float,\n",
      "    sumPhotonEtHighThreshold : Float,\n",
      "    sumPUPt : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record27 (\n",
      "    sumChargedHadronPt : Float,\n",
      "    sumChargedParticlePt : Float,\n",
      "    sumNeutralHadronEt : Float,\n",
      "    sumPhotonEt : Float,\n",
      "    sumNeutralHadronEtHighThreshold : Float,\n",
      "    sumPhotonEtHighThreshold : Float,\n",
      "    sumPUPt : Float\n",
      ")\n",
      "\n",
      "\n",
      "case class Record29 (\n",
      "    fX : Double,\n",
      "    fY : Double,\n",
      "    fZ : Double,\n",
      "    fT : Double\n",
      ")\n",
      "\n",
      "\n",
      "case class Record28 (\n",
      "    fCoordinates : Record29\n",
      ")\n",
      "\n",
      "\n",
      "case class Record0 (\n",
      "    qx3_ : Int,\n",
      "    pt_ : Float,\n",
      "    eta_ : Float,\n",
      "    phi_ : Float,\n",
      "    mass_ : Float,\n",
      "    vertex_ : Record1,\n",
      "    pdgId_ : Int,\n",
      "    status_ : Int,\n",
      "    innerTrack_ : Record3,\n",
      "    outerTrack_ : Record5,\n",
      "    globalTrack_ : Record7,\n",
      "    recoMuons_muons__RECO_obj_refittedTrackMap_ : Map[Int, Record9],\n",
      "    bestTrackType_ : Int,\n",
      "    calEnergy_ : Record11,\n",
      "    combinedQuality_ : Record18,\n",
      "    time_ : Record23,\n",
      "    energyValid_ : Boolean,\n",
      "    matchesValid_ : Boolean,\n",
      "    isolationValid_ : Boolean,\n",
      "    pfIsolationValid_ : Boolean,\n",
      "    qualityValid_ : Boolean,\n",
      "    caloCompatibility_ : Float,\n",
      "    isolationR03_ : Record24,\n",
      "    isolationR05_ : Record25,\n",
      "    pfIsolationR03_ : Record26,\n",
      "    pfIsolationR04_ : Record27,\n",
      "    type_ : Int,\n",
      "    pfP4_ : Record28\n",
      ")\n",
      "\n",
      "\n",
      "case class Event (\n",
      "    muons : Seq[Record0]\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val queue = dsMuons.schema.codeGen(\"Event\")\n",
    "val s = queue.mkString(\"\\n\")\n",
    "println(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Record2 (\n",
    "    fX : Float,\n",
    "    fY : Float,\n",
    "    fZ : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record1 (\n",
    "    fCoordinates : Record2\n",
    ")\n",
    "\n",
    "\n",
    "case class Record4 (\n",
    "    processIndex_ : Short,\n",
    "    productIndex_ : Short,\n",
    "    elementIndex_ : Int\n",
    ")\n",
    "\n",
    "\n",
    "case class Record3 (\n",
    "    recoMuons_muons__RECO_obj_innerTrack__product_ : Record4\n",
    ")\n",
    "\n",
    "\n",
    "case class Record6 (\n",
    "    processIndex_ : Short,\n",
    "    productIndex_ : Short,\n",
    "    elementIndex_ : Int\n",
    ")\n",
    "\n",
    "\n",
    "case class Record5 (\n",
    "    recoMuons_muons__RECO_obj_outerTrack__product_ : Record6\n",
    ")\n",
    "\n",
    "\n",
    "case class Record8 (\n",
    "    processIndex_ : Short,\n",
    "    productIndex_ : Short,\n",
    "    elementIndex_ : Int\n",
    ")\n",
    "\n",
    "\n",
    "case class Record7 (\n",
    "    recoMuons_muons__RECO_obj_globalTrack__product_ : Record8\n",
    ")\n",
    "\n",
    "\n",
    "case class Record10 (\n",
    "    processIndex_ : Short,\n",
    "    productIndex_ : Short,\n",
    "    elementIndex_ : Int\n",
    ")\n",
    "\n",
    "\n",
    "case class Record9 (\n",
    "    product_ : Record10\n",
    ")\n",
    "\n",
    "\n",
    "case class Record13 (\n",
    "    fX : Float,\n",
    "    fY : Float,\n",
    "    fZ : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record12 (\n",
    "    fCoordinates : Record13\n",
    ")\n",
    "\n",
    "\n",
    "case class Record15 (\n",
    "    fX : Float,\n",
    "    fY : Float,\n",
    "    fZ : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record14 (\n",
    "    fCoordinates : Record15\n",
    ")\n",
    "\n",
    "\n",
    "case class Record16 (\n",
    "    id_ : Int\n",
    ")\n",
    "\n",
    "\n",
    "case class Record17 (\n",
    "    id_ : Int\n",
    ")\n",
    "\n",
    "\n",
    "case class Record11 (\n",
    "    tower : Float,\n",
    "    towerS9 : Float,\n",
    "    em : Float,\n",
    "    emS9 : Float,\n",
    "    emS25 : Float,\n",
    "    emMax : Float,\n",
    "    had : Float,\n",
    "    hadS9 : Float,\n",
    "    hadMax : Float,\n",
    "    ho : Float,\n",
    "    hoS9 : Float,\n",
    "    ecal_time : Float,\n",
    "    ecal_timeError : Float,\n",
    "    hcal_time : Float,\n",
    "    hcal_timeError : Float,\n",
    "    ecal_position : Record12,\n",
    "    hcal_position : Record14,\n",
    "    ecal_id : Record16,\n",
    "    hcal_id : Record17\n",
    ")\n",
    "\n",
    "\n",
    "case class Record20 (\n",
    "    fX : Double,\n",
    "    fY : Double,\n",
    "    fZ : Double\n",
    ")\n",
    "\n",
    "\n",
    "case class Record19 (\n",
    "    fCoordinates : Record20\n",
    ")\n",
    "\n",
    "\n",
    "case class Record22 (\n",
    "    fX : Double,\n",
    "    fY : Double,\n",
    "    fZ : Double\n",
    ")\n",
    "\n",
    "\n",
    "case class Record21 (\n",
    "    fCoordinates : Record22\n",
    ")\n",
    "\n",
    "\n",
    "case class Record18 (\n",
    "    updatedSta : Boolean,\n",
    "    trkKink : Float,\n",
    "    glbKink : Float,\n",
    "    trkRelChi2 : Float,\n",
    "    staRelChi2 : Float,\n",
    "    chi2LocalPosition : Float,\n",
    "    chi2LocalMomentum : Float,\n",
    "    localDistance : Float,\n",
    "    globalDeltaEtaPhi : Float,\n",
    "    tightMatch : Boolean,\n",
    "    glbTrackProbability : Float,\n",
    "    tkKink_position : Record19,\n",
    "    glbKink_position : Record21\n",
    ")\n",
    "\n",
    "\n",
    "case class Record23 (\n",
    "    nDof : Int,\n",
    "    timeAtIpInOut : Float,\n",
    "    timeAtIpInOutErr : Float,\n",
    "    timeAtIpOutIn : Float,\n",
    "    timeAtIpOutInErr : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record24 (\n",
    "    sumPt : Float,\n",
    "    emEt : Float,\n",
    "    hadEt : Float,\n",
    "    hoEt : Float,\n",
    "    nTracks : Int,\n",
    "    nJets : Int,\n",
    "    trackerVetoPt : Float,\n",
    "    emVetoEt : Float,\n",
    "    hadVetoEt : Float,\n",
    "    hoVetoEt : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record25 (\n",
    "    sumPt : Float,\n",
    "    emEt : Float,\n",
    "    hadEt : Float,\n",
    "    hoEt : Float,\n",
    "    nTracks : Int,\n",
    "    nJets : Int,\n",
    "    trackerVetoPt : Float,\n",
    "    emVetoEt : Float,\n",
    "    hadVetoEt : Float,\n",
    "    hoVetoEt : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record26 (\n",
    "    sumChargedHadronPt : Float,\n",
    "    sumChargedParticlePt : Float,\n",
    "    sumNeutralHadronEt : Float,\n",
    "    sumPhotonEt : Float,\n",
    "    sumNeutralHadronEtHighThreshold : Float,\n",
    "    sumPhotonEtHighThreshold : Float,\n",
    "    sumPUPt : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record27 (\n",
    "    sumChargedHadronPt : Float,\n",
    "    sumChargedParticlePt : Float,\n",
    "    sumNeutralHadronEt : Float,\n",
    "    sumPhotonEt : Float,\n",
    "    sumNeutralHadronEtHighThreshold : Float,\n",
    "    sumPhotonEtHighThreshold : Float,\n",
    "    sumPUPt : Float\n",
    ")\n",
    "\n",
    "\n",
    "case class Record29 (\n",
    "    fX : Double,\n",
    "    fY : Double,\n",
    "    fZ : Double,\n",
    "    fT : Double\n",
    ")\n",
    "\n",
    "\n",
    "case class Record28 (\n",
    "    fCoordinates : Record29\n",
    ")\n",
    "\n",
    "\n",
    "case class Record0 (\n",
    "    qx3_ : Int,\n",
    "    pt_ : Float,\n",
    "    eta_ : Float,\n",
    "    phi_ : Float,\n",
    "    mass_ : Float,\n",
    "    vertex_ : Record1,\n",
    "    pdgId_ : Int,\n",
    "    status_ : Int,\n",
    "    innerTrack_ : Record3,\n",
    "    outerTrack_ : Record5,\n",
    "    globalTrack_ : Record7,\n",
    "    recoMuons_muons__RECO_obj_refittedTrackMap_ : Map[Int, Record9],\n",
    "    bestTrackType_ : Int,\n",
    "    calEnergy_ : Record11,\n",
    "    combinedQuality_ : Record18,\n",
    "    time_ : Record23,\n",
    "    energyValid_ : Boolean,\n",
    "    matchesValid_ : Boolean,\n",
    "    isolationValid_ : Boolean,\n",
    "    pfIsolationValid_ : Boolean,\n",
    "    qualityValid_ : Boolean,\n",
    "    caloCompatibility_ : Float,\n",
    "    isolationR03_ : Record24,\n",
    "    isolationR05_ : Record25,\n",
    "    pfIsolationR03_ : Record26,\n",
    "    pfIsolationR04_ : Record27,\n",
    "    type_ : Int,\n",
    "    pfP4_ : Record28\n",
    ")\n",
    "\n",
    "\n",
    "case class Event (\n",
    "    muons : Seq[Record0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the muon dataset from Dataset[Row] to Dataset[Event], where Event is the top level case class generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsMuonsSchema = dsMuons.as[Event]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muon object selection\n",
    "\n",
    "Only some of the basic kinematic and isolation cuts were able to be implemented.  Most of the detailed identification cuts could not be implemented, as they rely on track references.\n",
    "\n",
    "### Implemented\n",
    "\n",
    "-  pT(μ) > 10 GeV \n",
    "-  |η(μ)| < 2.4 \n",
    "-  Global Muon \n",
    "-  rel. ISO. < 0.5 (β = 0.5)\n",
    "\n",
    "### Not Implemented\n",
    "\n",
    "-  PF Muon \n",
    "-  chi2/ndf < 10 \n",
    "-  standalone hits > 0 \n",
    "-  matched stations >= 2 \n",
    "-  pixel hits > 0 \n",
    "-  N tracker layers >= 6 \n",
    "-  |dxy| < 0.2 cm \n",
    "-  |dz| < 0.5 cm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// helper function to calculate isolation from the struct\n",
    "def iso(isoStruct: Record27):Float = {\n",
    "    val neutral = max(0.0F, isoStruct.sumNeutralHadronEt + isoStruct.sumPhotonEt - 0.5F * isoStruct.sumPUPt)\n",
    "    isoStruct.sumChargedHadronPt + neutral\n",
    "}\n",
    "\n",
    "// just do the looser muon selection here.  will do a second selection for leading muon later\n",
    "def passMuonSel(muon: Record0):Boolean = {\n",
    "    (muon.pt_ > 10.0F) &&\n",
    "    (abs(muon.eta_) < 2.4F) &&\n",
    "    (iso(muon.pfIsolationR04_)/muon.pt_ < 0.5F) &&\n",
    "    ((muon.type_ & (1<<1)) != 0) // global muon    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return a Dataset[Event], where only the passing muons are kept in each event.  No events are removed at this stage - that comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsMuonsSel = dsMuonsSchema.map{\n",
    "    event =>\n",
    "    val pass_muons = event.muons.filter(muon => passMuonSel(muon))\n",
    "    Event(pass_muons)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Level Selection\n",
    "\n",
    "First require >= 2 muons to reduce the size quickly.  This only keeps events with at least 2 muons, where the muons already have the object level selection above applied to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1821,93)\n"
     ]
    }
   ],
   "source": [
    "val dsDimuons = dsMuonsSel.filter(event => event.muons.length > 1)\n",
    "println(dsMuonsSel.count,dsDimuons.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented\n",
    "\n",
    "- leading pT(μ) > 25 GeV \n",
    "- leading |η(μ)| < 2.1 \n",
    "- Leading two muons have opposite sign, used to compute invariant mass\n",
    "\n",
    "### Not Implemented\n",
    "\n",
    "- Pass trigger (too complicated to access)\n",
    "- |Y(Z)| < 2.2 (possible but need extra utility code to calculate)\n",
    "- select pair with mass closest to m(Z) -> instead choosing 2 highest pt muons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// method to apply the event level selection cuts,\n",
    "//  including tighter cuts on the leading muon\n",
    "def passEventSel(event:Event):Boolean = {\n",
    "    (event.muons(0).pt_ > 25.0F) &&\n",
    "    (abs(event.muons(0).eta_) < 2.1F) &&\n",
    "    (iso(event.muons(0).pfIsolationR04_)/event.muons(0).pt_ < 0.12F) &&\n",
    "    (event.muons(0).pdgId_ * event.muons(1).pdgId_ < 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This keeps only the event that pass the further selections from passEventSel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93,56)\n"
     ]
    }
   ],
   "source": [
    "val dsDimuonsSel = dsDimuons.filter(event => passEventSel(event))\n",
    "println(dsDimuons.count,dsDimuonsSel.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate invariant mass, store in parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invariantMass(mu1:Record0,mu2:Record0):Float = {\n",
    "    val pt1 = mu1.pt_\n",
    "    val phi1 = mu1.phi_\n",
    "    val eta1 = mu1.eta_\n",
    "    val pt2 = mu2.pt_\n",
    "    val phi2 = mu2.phi_\n",
    "    val eta2 = mu2.eta_\n",
    "    // simplified formula, assuming E >> m\n",
    "    sqrt(2*pt1*pt2*(cosh(eta1-eta2)-cos(phi1-phi2))).toFloat\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every event, calculates the invariant mass of the 2 leading leptons in pt, and stores that in a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsMll = dsDimuonsSel.map{\n",
    "    event => invariantMass(event.muons(0),event.muons(1))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost, executor driver): java.lang.ClassCastException: $line67.$read$$iw$$iw$Event cannot be cast to $line67.$read$$iw$$iw$Event\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:637)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:596)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:605)\n",
       "  ... 48 elided\n",
       "Caused by: java.lang.ClassCastException: Event cannot be cast to Event\n",
       "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsMll.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes out a parquet file with the Dataset of invariant mass values.  Note that this defines a directory, not a single file, and it will NOT overwrite an existing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find ROOT clas: vector<edm::StoredProductProvenance>. Swallowing...\n",
      "Could not find ROOT clas: vector<unsigned short>. Swallowing...\n"
     ]
    }
   ],
   "source": [
    "dsMll.write.format(\"parquet\").save(\"file:/home/olivito/datasci/spark/mll_AOD.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
